             Enabling Programmable Transport Protocols in High-Speed NICs

    Mina Tahmasbi Arashloo                   Alexey Lavrov              Manya Ghobadi                Jennifer Rexford
     Princeton University                 Princeton University              MIT                    Princeton University
                                     David Walker                     David Wentzlaff
                                  Princeton University              Princeton University


Abstract                                                          to the NIC to either be used directly through the socket API
                                                                  (TCP Offload Engine [10]) or to enable RDMA (iWARP [7]).
   Data-center network stacks are moving into hardware to
                                                                     These protocols, however, only use a small fixed set
achieve 100 Gbps data rates and beyond at low latency and
                                                                  out of the myriad of possible algorithms for reliable deliv-
low CPU utilization. However, hardwiring the network stack
                                                                  ery [16, 21, 24, 27, 33, 34] and congestion control [12, 17, 19,
in the NIC would stifle innovation in transport protocols. In
                                                                  35, 42, 43] proposed over the past few decades. For instance,
this paper, we enable programmable transport protocols in
                                                                  recent work suggests that low-latency data-center networks
high-speed NICs by designing Tonic, a flexible hardware ar-
                                                                  can significantly benefit from receiver-driven transport pro-
chitecture for transport logic. At 100 Gbps, transport pro-
                                                                  tocols [21,24,36], which is not an option in today’s hardware
tocols must generate a data segment every few nanoseconds
                                                                  stacks. In an attempt to deploy RoCE NICs in Microsoft data
using only a few kilobits of per-flow state on the NIC. By
                                                                  centers, operators needed to modify the data delivery algo-
identifying common patterns across transport logic of dif-
                                                                  rithm to avoid livelocks in their network but had to rely on
ferent transport protocols, we design an efficient hardware
                                                                  the NIC vendor to make that change [22]. Other algorithms
“template” for transport logic that satisfies these constraints
                                                                  have been proposed to improve RoCE’s simple reliable deliv-
while being programmable with a simple API. Experiments
                                                                  ery algorithm [31, 34]. The long list of optimizations in TCP
with our FPGA-based prototype show that Tonic can support
                                                                  from years of deployment in various networks is a testament
the transport logic of a wide range of protocols and meet tim-
                                                                  to the need for programmability in transport protocols.
ing for 100 Gbps of back-to-back 128-byte packets. That is,
every 10 ns, our prototype generates the address of a data           In this paper, we investigate how to make hardware trans-
segment for one of more than a thousand active flows for a        port protocols programmable. Even if NIC vendors open
downstream DMA pipeline to fetch and transmit a packet.           up interfaces for programming their hardware, it takes a sig-
                                                                  nificant amount of expertise, time, and effort to implement
1   Introduction                                                  transport protocols in high-speed hardware. To keep up with
                                                                  100 Gbps, the transport protocol should generate and trans-
   Transport protocols, along with the rest of the network        mit a packet every few nanoseconds. It should handle more
stack, traditionally run in software. Despite efforts to im-      than a thousand active flows, typical in today’s data-center
prove their performance and efficiency [1,6,25,32], software      servers [15, 37, 38]. To make matters worse, NICs are ex-
network stacks tend to consume 30-40% of CPU cycles to            tremely constrained in terms of the amount of their on-chip
keep up with applications in today’s data centers [25,32,38].     memory and computing resources [30, 34].
   As data centers move to 100 Gbps Ethernet, the CPU                We argue that transport protocols on high-speed NICs can
utilization of software network stacks becomes increasingly       be made programmable without exposing users to the full
prohibitive. As a result, multiple vendors have developed         complexity of programming for high-speed hardware. Our
hardware network stacks that run entirely on the network in-      argument is grounded in two main observations:
terface card (NIC) [8, 10]. However, there are only two main         First, programmable transport logic is the key to en-
transport protocols implemented on these NICs, both hard-         abling flexible hardware transport protocols. An imple-
wired and modifiable only by the vendors:                         mentation of a transport protocol performs several function-
   RoCE. RoCE is used for Remote Direct Memory Access             ality such as connection management, data buffer manage-
(RDMA) [8], using DCQCN [43] for congestion control and           ment, and data transfer. However, its central responsibility,
a simple go-back-N method for reliable data delivery.             where most of the innovation happens, is to decide which
   TCP. A few vendors offload a TCP variant of their choice       data segments to transfer (data delivery) and when (conges-

tion control), which we collectively call the transport logic.
Thus, the key to programmable transport protocols on high-
speed NICs is enabling users to modify the transport logic.
   Second, we can exploit common patterns in transport
logic to create reusable high-speed hardware modules.
Despite their differences in application-level API (e.g., sock-
ets and byte-stream abstractions for TCP vs. the message-          Figure 1: Tonic providing programmable transport logic in a hard-
                                                                   ware network stack on the NIC (sender-side).
based Verbs API for RDMA), and in connection and data
buffer management, transport protocols share several com-
                                                                   plementing transport logic on high-speed NICs (§2.2).
mon patterns. For instance, different transport protocols use
different algorithms to detect lost packets. However, once a       2.1       How Tonic Fits in the Transport Layer
packet is declared lost, reliable transport protocols prioritize
its retransmission over sending a new data segment. As an-            Sitting between applications and the rest of the stack,
other example, in congestion control, given the parameters         transport-layer protocols perform two main functions:
determined by the control loop (e.g., congestion window and        Connection Management includes creating and configuring
rate), there are only a few common ways to calculate how           endpoints (e.g., sockets for TCP and queue-pairs for RDMA)
many bytes a flow can transmit at any time. This enables us        and establishing the connection in the beginning, and closing
to design an efficient “template” for transport logic in hard-     the connection and releasing its resources at the end.
ware that can be programmed with a simple API.                     Data Transfer involves delivering data from one endpoint
   Using these insights, we design and develop Tonic, a pro-       to another, reliably and efficiently, in a stream of segments 1 .
grammable hardware architecture that can realize the trans-        Different transport protocols provide different APIs for ap-
port logic of a broad range of transport protocols, using a        plications to request data transfer: TCP offers the abstraction
simple API, while supporting 100 Gbps data-rates. Every            of a byte-stream to which applications can continuously ap-
clock cycle, Tonic generates the address of the next segment       pend data, while in RDMA, each “send” call to a queue-pair
for transmission. The data segment is fetched from memory          creates a separate work request and is treated as a separate
by a downstream DMA pipeline and turned into a full packet         message. Moreover, specifics of managing applications’ data
by the rest of the hardware network stack (Figure 1).              buffers differ across different implementations of transport
                                                                   protocols. Regardless, the transport protocol must deliver
   We envision that Tonic would reside on the NIC, re-
                                                                   the outstanding data to its destination in multiple data seg-
placing the hard-coded transport logic in hardware imple-
                                                                   ments that fit into individual packets. Deciding which bytes
mentations of transport protocols (e.g., future RDMA NICs
                                                                   comprise the next segment and when it is transmitted is done
and TCP offload engines). Tonic provides a unified pro-
                                                                   by data delivery and congestion control algorithms, which
grammable architecture for transport logic, independent of
                                                                   we collectively call transport logic and implement in Tonic.
how specific implementations of different transport proto-
                                                                      Figure 1 shows a high-level overview of how Tonic fits in a
cols perform connection and data buffer management, and
                                                                   hardware network stack. To decouple Tonic from specifics of
their application-level APIs. We will, however, describe how
                                                                   connection management and application-level APIs, connec-
Tonic interfaces with the rest of the transport layer in general
                                                                   tion setup and tear-down run outside of Tonic. Tonic relies
(§2) and how it can be integrated into Linux Kernel to inter-
                                                                   on the rest of the transport layer to provide it with a unique
act with applications using socket API as an example (§5).
                                                                   identifier (flow id) for each established connection, and to
   We implement a Tonic prototype in ∼8K lines of Ver-             explicitly add and remove connections using these IDs.
ilog code and demonstrate Tonic’s programmability by im-
                                                                      For data transfer on the sender side, Tonic keeps track of
plementing the transport logic of a variety of transport pro-
                                                                   the number of outstanding bytes and transport-specific meta-
tocols [13, 16, 23, 24, 34, 43] in less than 200 lines of code.
                                                                   data to implement the transport logic, i.e., generate the ad-
We also show, using an FPGA, that Tonic meets timing for
                                                                   dress of the next data segment for each flow at the time desig-
∼100 Mpps, i.e., supporting 100Gbps of back-to-back 128B
                                                                   nated by the congestion control algorithm. Thus, Tonic does
packets. That is, every 10ns, Tonic can generate the transport
                                                                   not need to store and/or handle actual data bytes; it relies
metadata required for a downstream DMA pipeline to fetch
                                                                   on the rest of the transport layer to manage data buffers on
and send one packet. From generation to transmission, the
                                                                   the host, DMA the segment whose address is generated in
latency of a single segment address through Tonic is ∼ 0.1µs,
                                                                   Tonic from memory, and notify it of new requests for data
and Tonic can support up to 2048 concurrent flows.
                                                                   transmission on existing connections (see §5 for details).
                                                                      The receiver-side of transport logic mainly involves gen-
2   Tonic as the Transport Logic                                   erating control signals such as acknowledgments, per-packet
  This section is an overview of how Tonic fits into the trans-       1 Wefocus on reliable transport as it is more commonly used and more
port layer (§2.1), and how it overcomes the challenges of im-      complicated to implement.

 #                Observation                        Examples          these modules for timing and memory, while simplifying the
 1 Only track a limited window of segments     TCP, NDP, IRN
 2 Only keep a few bits of state per segment   TCP, NDP, IRN, RoCEv2
                                                                       programming API by reducing the functionality users must
 3 Lost segments first, new segments next      TCP, NDP, IRN, RoCEv2   specify. These patterns are summarized in Table 1, and are
 4 Loss detection: Acks and timeouts           TCP, NDP, IRN           discussed in detail in next section, where we describe Tonic’s
   The three common credit calculation                                 components and how these patterns affect their design.
 5                                             TCP, RoCEv2, NDP
   patterns: window, rate, and grant tokens
            Table 1: Common transport logic patterns.                  3     Tonic Architecture
grant tokens [21, 24, 36], or periodic congestion notification            Transport logic at the sender is what determines, for each
packets (CNPs) [43], while the rest of the transport layer             flow, which data segments to transfer (data delivery) and
manages receive data buffers and delivers the received data            when (congestion control). Conceptually, congestion con-
to applications. While handling received data can get quite            trol algorithms perform credit management, i.e., determine
complicated, generating control signals on the receiver is             how many bytes a given flow can transmit at a time. Data
typically simpler than the sender. Thus, although we mainly            delivery algorithms perform segment selection, i.e., decide
focus on the sender, we reuse modules from the sender to               which contiguous sequence of bytes a particular flow should
implement a receiver solely for generating per-packet cumu-            transmit. Although the terms “data delivery” and “con-
lative and selective acks and grant tokens at line rate.               gestion control” are commonly associated with TCP-based
                                                                       transport protocols, Tonic provides a general programmable
2.2    Hardware Design Challenges                                      architecture for transport logic that can be used for other
                                                                       kinds of transport protocols as well, such as receiver-driven
   Implementing transport logic at line rate in the NIC is
                                                                       [21, 24, 36] and RDMA-based [8] transport protocols.
challenging due to two main constraints:
                                                                          Tonic exploits the natural functional separation between
   Timing constraints. The median packet size in data cen-
                                                                       data delivery and credit management to partition them into
ters is less than 200 bytes [15, 37]. To achieve 100 Gbps
                                                                       two components with separate state (Figure 2). The data de-
for these small packets, the NIC has to send a packet every
                                                                       livery engine processes events related to generating, track-
∼10 ns. Thus, every ∼10 ns, the transport logic should deter-
                                                                       ing, and delivery of segments, while the credit engine pro-
mine which active flow should transmit which data segment
                                                                       cesses events related to adjusting each flow’s credit and send-
next. To make this decision, it uses some state per flow (e.g.,
                                                                       ing out segment addresses for those with sufficient credit.
acknowledged data segments, duplicate acks, rate/window
size, etc.) which is updated when various transport events                At the cost of lightweight coordination between the two
happen (e.g., receiving an acknowledgment or a timeout).               engines, this partitioning helps Tonic meet its timing con-
These updates could involve operations with non-negligible             straints while concurrently processing multiple events (e.g.,
hardware overhead, such as searching bitmaps and arrays.               receipt of acknowledgments and segment transmission) ev-
                                                                       ery cycle. These events must read the current state of their
   To allow for more time in processing each event while
                                                                       corresponding flow, update it, and write it back to memory
still determining the next data segment every ∼10 ns, we
                                                                       for events in the next cycle. However, concurrent read and
could conceivably pipeline the processing of transport events
                                                                       write to memory in every cycle is costly. Instead of using a
across multiple stages. However, pipelining is more tractable
                                                                       wide memory to serve all the transport events, the partition-
when incoming events are from different flows as they up-
                                                                       ing allows the data delivery and credit engines to have nar-
date different states. Processing back-to-back events for the
                                                                       rower memories to serve only the events that matter for their
same flow (e.g., generating data segments while receiving ac-
                                                                       specific functionality, hence meeting timing constraints.
knowledgments) requires updates to the same state, making
it difficult to pipeline event processing while ensuring state            In this section, we present, in §3.1, how the engines co-
consistency. Thus, we strive to process each transport event           ordinate to fairly and efficiently pick one of a few thou-
within 10 ns instead to quickly consolidate the state for the          sand flows every cycle for segment transmission while keep-
next event in case it affects the same flow.                           ing the outgoing link utilized. Next, §3.2 and §3.3 describe
                                                                       fixed-function and programmable event processing modules
   Memory constraints. A typical data-center server has
                                                                       in each engine, and how their design is inspired by patterns
more than a thousand concurrent active flows with kilobytes
                                                                       in Table 1. We present Tonic’s solution for resolving con-
of in-flight data [15, 37, 38]. Since NICs have just a few
                                                                       flicts when multiple events for the same flow are received in
megabytes of high-speed memory [30,34], the transport pro-
                                                                       a cycle in §3.4, and its programming interface in §3.5.
tocol can store only a few kilobits of state per flow on NIC.
   Tonic’s goal is to satisfy these tight timing and memory
                                                                       3.1    Efficient Flow Scheduling
constraints while supporting programmability with a simple
API. To do so, we identify common patterns across trans-                  At any time, a flow can only transmit a data segment if
port logic in various protocols that we implement as reusable          it (1) has enough credit, and (2) has a new or lost segment
fixed-function modules. These patterns allow us to optimize            to send. To be work conserving, Tonic must track the set

                Figure 2: Tonic’s architecture (dark red boxes (also with thick borders) are programmable, others are fixed)

of flows that are eligible for transmission (meet both of the            will be inserted in the set if, later on, the receipt of an ack or
above criteria) and only pick among those when selecting a               a signal from the credit engine “activates” the flow (Step 9).
flow for transmission each cycle. This is challenging to do              Moreover, the generated segment address is forwarded to the
efficiently. We have more than a thousand flows with their               credit engine (Step 4) for insertion in the ring buffer (Step 5).
state partitioned across the two engines: Only the credit en-               Similarly, the credit engine maintains the set of ready-to-
gine knows how much credit a flow has, and only the data de-             transmit flows, i.e., the flows with one segment address or
livery engine knows the status of a flow’s segments and can              more in their ring buffers and enough credit to send at least
generate the address of its next segment. We cannot check                one segment out. Every cycle, a flow is selected from the set
the state of all the flows every cycle across both engines to            (Step 6), one segment address from its ring buffer is trans-
find the ones eligible for transmission in that cycle.                   mitted (Step 7), its credit is decreased, and it is inserted back
   Instead, we decouple the generation of segment addresses              into the set if it has more segment addresses and credit for
from their final transmission to the DMA pipeline. We al-                further transmission (Step 8). It also signals the data deliv-
low the data delivery engine to generate up to N segment ad-             ery engine about the transmission (Step 9) to decrement the
dresses for a flow without necessarily having enough credit              number of outstanding segments for that flow.
to send them out. In the credit engine, we keep a ring buffer               To be fair when picking flows from the active (or ready-to-
of size N for each flow to store these outstanding segments              transmit) set, Tonic uses a FIFO to implement round-robin
addresses. When the flow has enough credit to send a seg-                scheduling among flows in the set (see active list in [39]).
ment, the credit engine dequeues and outputs a segment ad-               The choice of round-robin scheduling is not fundamental;
dress from the buffer and signals the data delivery engine to            any other scheduler that meets our timing constraints can re-
decrement the number of outstanding segments for that flow.              place the FIFO to support other scheduling disciplines [40].
   This solves the problem of the partitioned state across the
two engines. The data delivery engine does not need to keep
                                                                         3.2    Flexible Segment Selection
track of the credit changes of flows for segment address gen-               With B bytes of credit, a flow can send S = max(B, MSS)
eration. It only needs to be notified when a segment address             bytes, where MSS is the maximum segment size. In transport
is dequeued from the buffer. Moreover, the credit engine                 protocols, data delivery algorithms use acknowledgments to
does not need to know the exact status of all flow’s segments.           keep track of the status of each byte of data (e.g., delivered,
If the flow’s ring buffer is empty, that flow does not have              lost, in-flight, and not transmitted), and use that to decide
segments to send. Otherwise, there are already segment ad-               which contiguous S bytes of data to transmit next.
dresses that can be output when the flow has enough credit.                 However, there are two main challenges in implementing
   Still, the data delivery engine cannot simply check the               data delivery algorithms in high-speed NICs. First, due to
state of all the flows every cycle to determine those that can           memory constraints, the NIC cannot store per-byte informa-
generate segments. Instead, we dynamically maintain the set              tion. Second, with a few exceptions [8, 34], these algorithms
of active flows in the data delivery engine, i.e., the flows that        are designed for software, where they could store and freely
have at least one segment to generate and less than N out-               loop through large arrays of metadata to aggregate informa-
standing segments (see red numbered circles in Figure 2).                tion. This computational flexibility has created significant
When a flow is created, it is added to the active set. Every             diversity across these algorithms. Unfortunately, NIC hard-
cycle, one flow is selected and removed from the set for seg-            ware is much more constrained than software. Thus, we did
ment generation (Step 1). Once processed (Step 2), only if it            not aim to support all data delivery algorithms. Instead, we
has more segments to send and less than N outstanding seg-               looked for patterns that are common across a variety of algo-
ments, is it inserted back into the set (Step 3). Otherwise, it          rithms while being amenable to hardware implementation.

3.2.1    Pre-Calculated Fixed Segment Boundaries                                 pute the evidence for the rest as needed. Based on these
                                                                                 observations (#1 and #2 in Table 1), we use a fixed set of
   Data delivery algorithms could conceivably choose the
                                                                                 bitmaps in Tonic’s data delivery engine to track the status of
next S bytes to send from anywhere in the data stream and
                                                                                 a flow’s segments and implement optimized fixed-function
produce segments with variable boundaries. However, since
                                                                                 bitmap operations for updating them on transport events.
the NIC cannot maintain per-byte state, Tonic requires data
to be partitioned into fixed-size segments (by a Kernel mod-                     3.2.3   Concurrent Event Processing
ule or the driver, see §5) when the flow requests transmission
of new data. This way, data delivery algorithms can use per-                        For every flow, four main events can affect the generation
segment information to select the next segment.                                  of its next segment address. First, the receipt of an acknowl-
                                                                                 edgment can either move the window forward and enable the
   Note that the fixed segment size can be configured for each
                                                                                 flow to generate more segments, or signal segment loss and
flow based on its throughput and latency requirements. With
                                                                                 trigger retransmissions. Second, the absence of acknowledg-
message-based transport protocols (e.g., RoCEv2), having
                                                                                 ments, i.e., a timeout, can also lead to more segments marked
fixed segment boundaries fits naturally; the message length
                                                                                 as lost and trigger retransmissions. Third, generation of a
is known and the optimal segment size can be chosen from
                                                                                 segment address increments the number of a flow’s outstand-
the beginning. For protocols with a byte-stream abstraction
                                                                                 ing segments and can deactivate the flow if it goes above N.
(e.g., TCP and NDP), the fixed segment size should be de-
                                                                                 Fourth, segment address transmission (out of the credit en-
cided on the fly as data is added to the stream. It can be set to
                                                                                 gine) decrements the number of outstanding segments and
MSS (or larger if using TSO [18]) for high-bandwidth flows.
                                                                                 can enable the flow to generate more segment addresses.
For flows that generate small data segments and sporadically,
                                                                                    Tonic’s data delivery engine has four modules to handle
the segment size can be set to a smaller value, depending
                                                                                 these four events (Figure 2). Every cycle, each module reads
on whether it is more desirable to consolidate multiple small
                                                                                 the state of the flow for which it received an event from the
segments into a larger one before notifying Tonic, or to trans-
                                                                                 memory in the data delivery engine, processes the event, and
mit the small segment right away (§5). Regardless, to avoid
                                                                                 updates the flow state accordingly. The flow state in the data
storing per-byte state on the NIC, segment size should be de-
                                                                                 delivery engine consists of a fixed set of variables to track the
cided outside of Tonic and changed infrequently.
                                                                                 status of the current window of segments across events, as
3.2.2    Small Per-Segment State for a Limited Window                            well as the user-defined variables used in the programmable
                                                                                 components (Table 2). As an example of the fixed state vari-
   Independent of a flow’s available credit, data delivery al-                   ables, Tonic keeps a fixed set of bitmaps for each flow (ob-
gorithms typically do not transmit a new segment if it is too                    servations in §3.2.2): The acked bitmap keeps track of selec-
far, i.e., more than K segments apart, from the first unac-                      tively acknowledged segments, marked-for-rtx keeps track
knowledged segment, to limit the state that the sender and                       of lost segments that require retransmission, and rtx-cnt
receiver need to keep 2 . Still, in a 100 Gbps network with a                    stores information about their previous retransmissions.
10µs RTT, K can get as large as ∼128 segments. Fortunately,
                                                                                    The following paragraphs briefly describe how each event-
we observe that storing the following per-segment state is
                                                                                 processing module affects a flow’s state, and whether there
enough for most data delivery algorithms: (1) Is the segment
                                                                                 are common patterns that we can exploit to implement all or
acknowledged (in presence of selective acknowledgments)?
                                                                                 parts of its functionality in a fixed-function manner.
(2) If not, is it lost or still in flight? (3) If lost, is it already
                                                                                    Incoming. This module processes acknowledgments (and
retransmitted (to avoid redundant retransmission)?
                                                                                 other incoming packets, see §3.3.3). Some updates to state
   More specifically, we observe that, in the absence of ex-                     variables in response to acknowledgments are similar across
plicit negative acknowledgments, data delivery algorithms                        all data delivery algorithms and do not need to be pro-
accumulate evidence of loss for each segment from posi-                          grammable (e.g., updating window boundaries, and mark-
tive acknowledgments, e.g., duplicate cumulative (e.g., TCP                      ing selectively acked segments in acked bitmap, see §3.2.2),
NewReno [23]) or selective acks (e.g., IRN for RDMA and                          whereas loss detection and recovery, which rely on acknowl-
TCP SACK [16]). Once the accumulated evidence for a seg-                         edgments as a signal, vary a lot across different algorithms
ment passes a threshold, the algorithm can declare it lost with                  and must be programmable by users (#4 in Table 1). Thus,
high confidence. Typically, an evidence of loss for segment                      the Incoming module is designed as a two-stage pipeline: a
i is also an evidence of loss for every unacknowledged seg-                      fixed-function stage for the common updates followed by a
ment j with j < i. Thus, most of these algorithms can be                         programmable stage for loss detection and recovery.
rewritten to only keep track of the total evidence of loss for
                                                                                    The benefit of this two-stage design is that the common
the first unacknowledged segment and incrementally com-
                                                                                 updates mostly involve bitmaps and arrays (§3.2.2), which
   2 In TCP-based protocols, K is the minimum of receive window and con-         are implemented as ring buffers in hardware and costly to
gestion window size. However, the limit imposed by K exists when transport       modify across their elements. For instance, in all data de-
protocols use other ways (e.g., rate) to limit a flow’s transmission pace [8].   livery algorithms, if an incoming packet acknowledges seg-

ment A cumulatively and segment S selectively, wnd-start is               State Variable      Description
                                                                          acked               selectively acknowledged segments (bitmap)
updated to max(wnd-start, A) and acked[S] to one, and the                 marked-for-rtx      lost segments marked for retransmission (bitmap)
boundaries of all bitmaps and arrays are updated based on the             rtx-cnt             number of retransmissions of a segment (bitmap)
new wnd-start. By moving these updates into a fixed func-                 wnd-start           the address of the first segment in the window
tion stage, we can (i) optimize them to meet Tonic’s timing               wnd-size            size of the window (min(W, rcved window))
                                                                          highest-sent        the highest segment transmitted so far
and memory constraints, and (ii) provide programmers with                 total-sent          Total number of segments transmitted so far
a dedicated stage, i.e., a separate cycle, to do loss detection           is-idle             does the flow have segments to send?
and recovery. In this dedicated stage, programmers can use                outstanding-cnt     # of outstanding segments
the updated state variables from the previous stage and the               rtx-timer           when will the rtx timer expire?
                                                                          user-context        user-defined variables for programmable modules
rest of the variables from memory to infer segment loss (and
perform other user-defined computation discussed in §3.3.3).                  Table 2: Per-flow state variables in the data delivery engine
   Periodic Updates. The data delivery engine iterates over            3.3.1      Common Credit-Calculation Patterns
active flows, sending them one at a time to this module to
check for retransmission timer expiration and perform other               Congestion control algorithms have a broad range of ways
user-defined periodic updates (§3.3.3). Thus, with its 10 ns           to estimate network capacity. However, they enforce limits
clock cycle, Tonic can cover each flow within a few mi-                on data transmission in three main ways (#5 in Table 1):
croseconds of the expiry of its retransmission timer. This             Congestion window. The control loop limits a flow to at
module must be programmable as a retransmission timeout                most W bytes in flight from the first unacknowledged byte.
is a signal for detecting loss (#4 in Table 1). Similar to the         Thus, if byte i is the first unacknowledged byte, the flow
programmable stage of the Incoming module, the program-                cannot send bytes beyond i + W . Keeping track of in-flight
mers can use per-flow state variables to infer segment loss.           segments to enforce a congestion window can get compli-
                                                                       cated, e.g., in the presence of selective acknowledgments,
   Segment Generation. Given an active flow and its vari-
                                                                       and is implemented in the fixed-function stage of the incom-
ables, this module generates the next segment’s address and
                                                                       ing module in the data delivery engine.
forwards it to the credit engine. Tonic can implement seg-
                                                                       Rate. The control loop limits the flow’s average rate (R) and
ment address generation as a fixed function module based on
                                                                       maximum burst size (D). Thus, if a flow had credit c0 at
the following observation (#3 in Table 1): Although different
                                                                       the time t0 of the last transmission, then the credit at time t
reliable data delivery algorithms have different ways of infer-
                                                                       will be min(R ∗ (t − t0 ) + c0 , D). As we show in §4, imple-
ring segment loss, once a lost segment is detected, it is only
                                                                       menting precise per-flow rate limiters under our strict timing
logical to retransmit it before sending anything new. Thus,
                                                                       and memory constraints is challenging and has an optimized
the procedure for selecting the next segment is the same irre-
                                                                       fixed-function implementation in Tonic.
spective of the data delivery algorithm, and is implemented
                                                                       Grant tokens. Instead of estimating network capacity, the
as a fixed-function module in Tonic. Thus, this module pri-
                                                                       control loop receives tokens from the receiver and adds them
oritizes retransmission of lost segments in marked-for-rtx
                                                                       to the flow’s credit. Thus, the credit of a flow is the total
over sending the next new segment, i.e., highest sent+1 and
                                                                       tokens received minus the number of transmitted bytes, and
also increments the number of outstanding segments.
                                                                       the credit calculation logic consists of a simple addition.
   Segment Transmitted. This module is fixed function and                 Since these are used by most congestion control algo-
is triggered when a segment address is transmitted out of the          rithms3 , we optimize their implementation to meet Tonic’s
credit engine. It decrements the number of outstanding seg-            timing and memory constraints. Congestion window calcu-
ments of the corresponding flow. If the flow was deactivated           lations are mostly affected by acks. Thus, calculation and
due to a full ring buffer, it is inserted into the active set again.   enforcement of congestion window happen in the data deliv-
                                                                       ery engine. For the other two credit calculation schemes, the
3.3    Flexible Credit Management                                      credit engine processes credit-related event, and Tonic users
                                                                       can simply pick which scheme to use in the credit engine.
   Transport protocols use congestion-control algorithms to
avoid overloading the network by controlling the pace of a             3.3.2      Event Processing for Credit Calculation
flow’s transmission. These algorithms consist of a control
loop that estimates the network capacity by monitoring the                Conceptually, three main events can trigger credit calcu-
stream of incoming control packets (e.g., acknowledgments              lation for a flow, and the credit engine has different modules
and congestion notification packets (CNPs)) and sets param-            to concurrently process them every cycle (Figure 2). First,
eters that limit outgoing data packets. While the control loop         when a segment address is received from the data delivery
is different in many algorithms, the credit calculation based          engine and is the only one in the flow’s ring buffer, the flow
on parameters is not. Tonic has efficient fixed-function mod-          could now qualify for transmission or remain idle based on
ules for credit calculation (§3.3.1 and §3.3.2) and relegates             3  Tonic’s credit engine has a modular event-based design (§3.3.2), mak-
parameter adjustment to programmable modules (§3.3.3).                 ing it amenable for extension to future credit calculation schemes.

its credit (the Enqueue module). Second, when a flow trans-                      As we show in §6.1.1, we have implemented several con-
mits a segment address, its credit must be decreased and we                   gestion control algorithms in Tonic and their parameter ad-
should determine whether it is qualified for further transmis-                justment calculations have finished within our 10 ns clock
sion based on its updated credit and the occupancy of its ring                cycle. Those with integer arithmetic operations did not need
buffer (the Transmit module). Third are events that can add                   any modifications. For those with floating-point operations,
credit to the flow (e.g., from grant tokens and leaky bucket                  such as DCQCN, we approximated the operations to a cer-
rate limiters), which is where the main difference lies be-                   tain decimal point using integer operations. If an algorithm
tween rate-based and token-based credit calculation.                          requires high-precision and complicated floating-point oper-
   When using grant tokens, the credit engine needs two ded-                  ations for parameter adjustment that cannot be implemented
icated modules to add credit to a flow: one to process incom-                 within one clock cycle [19], the computation can be rele-
ing grant tokens from the receiver, and one to add credit for                 gated to a floating-point arithmetic module outside of Tonic.
retransmissions on timeouts. When using rate, the credit en-                  This module can perform the computation asynchronously
gine does not need any extra modules for adding credit since                  and store the output in a separate memory, which merges
a flow with rate R bytes-per-cycle implicitly gains R bytes of                into Tonic through the “Periodic Updates” module.
credit every cycle and, therefore, we can compute in advance
when it will be qualified for transmission.                                   3.4    Handling Conflicting Events
   Suppose in cycle T0 , the Transmit module transmits a seg-
                                                                                 Tonic strives to process events concurrently in order to be
ment from flow f , and is determining whether the flow is
                                                                              responsive to events. Thus, if a flow receives more than one
qualified for further transmission. Suppose that f has more
                                                                              event in the same cycle, it allows the event processing mod-
segments in the ring buffer but lacks L bytes of credit. The
                                                                              ules to process the events and update the flow’s state vari-
Transmit module can compute when it will have sufficient
                                                                              ables, and reconciles the state before writing it back into
credit as T = RL and set up a timer for T cycles. When
                                                                              memory (the Merge modules in Figure 2).
the timer expires, f definitely has enough credit for at least
one segment, so it can be directly inserted into ready-to-tx.                    Since acknowledgments and retransmission timeouts are,
When f reaches the head of ready-to-tx and is processed                       by definition, mutually exclusive, Tonic discards the timeout
by the Transmit module again in cycle T1 , the Transmit mod-                  if it is received in the same cycle as an acknowledgment for
ule can increase f ’s credit by (T1 − T0 ) ∗ R − S, where S is                the same flow. This significantly simplifies the merge logic
the size of the segment that is transmitted at time T1 4 . Note               because several variables (window size and retransmission
that when using rate, the credit engine must perform division                 timer period) are only modified by these two events and,
and maintain per-flow timers. We will discuss the hardware                    therefore, are never updated concurrently. We can resolve
implementation of these operations in §4.                                     concurrent updates for the remaining variables with simple,
                                                                              predefined merge logic. For example, Segment Generation
3.3.3    Flexible Parameter Adjustment                                        increments the number of outstanding segments, whereas
   Congestion control algorithms often have a control loop                    Segment Transmitted decrements it; if both events affect the
that continuously monitors the network and adjusts credit                     same flow at the same time, the number does not change.
calculation parameters, i.e., rate or window size, based on                   User-defined variables are updated in either the Incoming or
estimated network capacity. Parameter adjustment is either                    the Periodic Updates module, and we rely on the program-
triggered by incoming packets (e.g., acknowledgments and                      mer to specify which updated variables should be prioritized
their signals such as ECN or delay in TCP variants and                        if both updates happen in the same cycle.
Timely, and congestion notification packets (CNPs) in DC-
QCN) or periodic timers and counters (timeouts in TCP vari-                   3.5    Tonic’s Programming Interface
ants and byte counter and various timers in DCQCN), and in                       To implement a new transport logic in Tonic, program-
some cases is inspired by segment losses as well (window                      mers only need to specify (i) which of the three credit man-
adjustment after duplicate acknowledgments in TCP).                           agement schemes to use, (ii) the loss detection and recovery
   Corresponding to these triggers, for specifying parameter                  logic in response to acknowledgments and timeouts, and (iii)
adjustment logic, Tonic’s users can use the programmable                      congestion-control parameter adjustment in response to in-
stage of the “Incoming” module, which sees all incoming                       coming packets or periodic timers and counters. The first one
packets, and the “Periodic Updates” module for timers and                     is used to pick the right modules for the credit engine, and the
counters. Both modules are in the data delivery engine and                    last two are inserted into the corresponding programmable
have access to segment status information, in case segment                    stages of the data delivery engine (Figure 2).
status (e.g., drops) is needed for parameter adjustment. The                     To specify the logic for the programmable stage of the In-
updated parameters are forwarded to the credit engine.                        coming module, programmers need to write a function that
   4 Similarly,the Enqueue module can set up the timer when it receives the   receives the incoming packet (ack or other control signals),
first segment of the queue and the flow lacks credit for its transmission.    the number of newly acknowledged segments, the acked

bitmap updated with the information in the ack, the old and        making it difficult to compute within our 10 ns target. In-
new value of wnd-start (in case the window moves forward           stead, Tonic uses a light-weight pre-processing on the input
due to a new cumulative ack), and the rest of the flow’s state     ring buffer to avoid head index computation in the layers al-
variables (Table 2) as input. In the output, they can mark a       together (details in Appendix C).
range of segments for retransmission in marked-for-rtx, up-        Concurrent Memory Access. Every cycle, five modules in
date congestion-control parameters such as window size and         the data delivery engine, including both stages of the Incom-
rate, and reset the retransmission timer. The programming          ing module, concurrently access its memory (§3.2.3). How-
interface of the Periodic Updates module is similar.               ever, FPGAs only have dual-ported block RAMs, with each
   In specifying these functions, programmers can use inte-        port capable of either read or write every cycle. Building
ger arithmetic operations, e.g., addition, subtraction, multi-     memories with more concurrent reads and writes requires
plication, and division with small-width operands, condition-      keeping multiple copies of data in separate memory “banks”
als, and a limited set of read-only bitmap operations, e.g., in-   and keeping track of the bank with the most recent data
dex lookup, and finding the first set bit in the updated acked     for each address5 [26]. To avoid supporting five concurrent
bitmap (see appendix F for an example program). Note that          reads and writes, we manage to partition per-flow state vari-
a dedicated fixed-function stage in the data delivery engine       ables into two groups, each processed by at most four events.
performs the costly common bitmap updates on receipt of            Thus, Tonic can use two memories with four read and write
acks (§3.2.3). We show, in §6.1.1, that a wide range of trans-     ports instead of a single one with five, to provide concurrent
port protocols can be implemented using this interface and         access for all processing modules at the same time.
give examples of ones that cannot.
                                                                   5    Integrating Tonic into the Transport Layer
4   Hardware Implementation
                                                                       Tonic’s transport logic is intentionally decoupled from
   In this section, we describe the hardware design of the         the specific implementation of other transport functionality
Tonic components that were the most challenging to imple-          such as connection management, application-level API, and
ment under Tonic’s tight timing and memory constraints.            buffer management. This section provides an example of
                                                                   how Tonic can interface with the Linux kernel to learn about
High-Precision Per-Flow Rate Limiting. A flow with rate
                                                                   new connections, requests for data transmission, and connec-
R bytes per cycle and L bytes to send will have sufficient
                                                                   tion termination 6 . After creating the socket, applications use
credit for transmission in T = d RL e cycles. Tonic needs to do
                                                                   various system calls for connection management and data
this computation in the credit engine but must represent R as
                                                                   transfer. As Tonic mainly focuses on the sender sider of the
an integer since it cannot afford to do floating-point division.
                                                                   transport logic, we only discuss the system calls and modifi-
This creates a trade-off between the rate-limiting precision
                                                                   cations relevant to the sender side of the transport layer.
and the range of rates Tonic can support. If R is in bytes per
cycle, we cannot support rates below one byte per cycle or         Connection Management. connect() on the client initiates
∼1 Gbps. If we represent R in bytes per thousand cycles,           a connection, listen() and accept() on the server listen for
we can support rates as low as 1 Mbps. However, T = d RL e         and accept connections, and close() terminate connections.
determines how many thousand cycles from now the flow              As connection management happens outside of Tonic, the
qualifies for transmission which results in lower rate con-        kernel implementation of these system calls stays untouched.
formance and precision for higher-bandwidth flows. To sup-         However, once the connection is established, the kernel maps
port a wide range of rates without sacrificing precision, Tonic    it to a unique flow id in [0, N), where N is the maximum num-
keeps multiple representations of the flow’s rate at different     ber of flows supported by Tonic, and notifies Tonic through
levels of precision and picks the most precise representation      the NIC driver about the new connection.
for computing T at any moment (details in Appendix B).                 Specifically, from the connection’s Transmission Control
                                                                   Block (TCB) in the kernel, the IP addresses and ports of
Efficient Bitmap Operations. Tonic uses bitmaps as large
                                                                   the communication endpoints are sent to Tonic alongside the
as 128 bits to track the status of segments for each flow.
                                                                   flow id and the fixed segment size chosen for the connec-
Bitmaps are implemented as ring buffers. The head pointer
                                                                   tion. The kernel only needs to track the TCB fields used for
corresponds to the first unacked segment and moves forward
                                                                   connection management (e.g., IP addresses, ports, and TCP
around the buffer with new acks. To efficiently implement
                                                                   FSM), pointers to data buffers, and receiver-related fields.
operations whose output depends on the values of all the bits
                                                                   Fields used for data transfer on the sender, i.e., snd.nxt,
in the bitmap, we must divide the buffer into smaller parts in
                                                                   snd.una, and snd.wnd, are stored in and handled by Tonic.
multiple layers, process them in parallel, and join the results.
                                                                   Finally, after a call to close(), the kernel uses the connec-
One such operation, frequently used in Tonic, is finding the
first set bit after the head. The moving head of the ring buffer       5 This overhead is specific to FPGAs, and can potentially be eliminated
complicates the implementation of this operation since keep-       if the memory is designed as an ASIC.
ing track of the head in each layer requires extra processing,         6 See appendix A for how Tonic can be used with RDMA.

tion’s flow id to notify Tonic of its termination.                design supports 2048 concurrent flows, matching the work-
Data Transfer. send() adds data to the connection’s socket        ing sets observed in data centers [15, 37] and other hardware
buffer, which stores its outstanding data waiting for delivery.   offloads in the literature [20]. If a host has more open con-
Tonic keeps a few bits of per-segment state for outstanding       nections than Tonic can support, the kernel can offload data
data and performs all transport logic computation in terms        transfer for flows to Tonic on a first-come first-serve basis,
of segments. As such, data should be partitioned into equal-      or have users set a flag when creating the socket and fall
sized segments before Tonic can start its transmission (§3.2).    back to software once Tonic runs out of resources for new
Thus, modifications to send() mainly involve determining          flows. Alternatively, modern FPGA-based NICs have a large
segment boundaries for the data in the socket buffer based on     DRAM directly attached to the FPGA [20]. The DRAM can
the connection’s configured segment size and deciding when        potentially be used to store the state of more connections,
to notify Tonic of the new segments. Specifically, the kernel     and swap them back and forth into Tonic’s memory as they
keeps an extra pointer for each connection’s socket buffer, in    activate and need to transmit data. Moreover, to provide visi-
addition to its head and tail, called tonic-tail. It points       bility into the performance of hardware transport logic, Tonic
to the last segment of which Tonic has been notified. head        can provide an interface for kernel to periodically pull trans-
and updates to tonic-tail are sent to Tonic to use when           port statistics from the NIC.
generating the next segment’s address to fetch from memory.       Other Transport Layers. The above design is an exam-
   Starting with an empty socket buffer, when the applica-        ple of how Tonic can be integrated into a commonly-used
tion calls send(), data is copied to the socket buffer, and       transport layer. However, TCP, sockets, and bytestreams
tail is updated accordingly. Assuming the connection’s            are not suitable for all applications. In fact, several data-
configured segment size is C, the data is then partitioned        center applications with high-bandwidth low-latency flows
into C-sized segments. Suppose the data is partitioned into       are starting to use RDMA and its message-based API in-
S segments and B < C remaining bytes. The kernel then             stead [5,9,22,35]. Tonic can be integrated into RDMA-based
updates tonic-tail to point to the end of the last C-sized        transport as well, which we discuss in appendix A.
segment, i.e., head + C * S, and notifies Tonic of the update
to tonic-tail. The extra B bytes remain unknown to Tonic          6   Evaluation
for a configurable time T , in case the application calls send
to provide more data. In that case, the data are added to the        To evaluate Tonic, we implement a prototype in Verilog
socket buffer, data between tonic-tail and tail are sim-          (∼8K lines of code) and a cycle-accurate hardware simulator
ilarly partitioned, tonic-tail is updated accordingly, and        in C++ (∼2K lines of code) [11]. The simulator is integrated
Tonic is notified of new data segments.                           with NS3 network simulator [4] for end-to-end experiments.
   If there is not enough data for a C-sized segment after time      To implement a transport protocol on Tonic’s Verilog pro-
T , the kernel needs to notify Tonic of the “sub-segment” (a      totype, programmers only need to provide three Verilog files:
segment smaller than C) and its size, and update tonic-tail       (i) incoming.v, describing the loss detection and recovery
accordingly. Note that Tonic requires all segments, except        logic and how to change credit management parameters (i.e.,
for the last one in a burst, to be of equal size, as all com-     rate or window) in response to incoming packets; this code
putations, including window updates, are in terms of seg-         is inserted into the second stage of the Incoming pipeline in
ments. Thus, after creating a “sub-segment”, if there is more     the data delivery engine, (ii) periodic updates.v, describ-
data from the application, Tonic can only start its trans-        ing the loss detection and recovery logic in response to time-
mission when it is done transferring its current segments.        outs and how to change credit management parameters (i.e.,
Tonic notifies the kernel once it successfully delivers the fi-   rate or window) in response to periodic timers and counters;
nal “sub-segment”, at which point, head and tonic-tail will       this code is inserted into the Periodic Updates module in the
be equal, and the kernel continues partitioning the remaining     data delivery engine, and (iii) user configs.vh, specifying
data in the socket buffer and updating Tonic as before. Note      which of the three credit calculation schemes to use and the
that Tonic can periodically, with a configurable frequency,       initial values of user-defined state variables and other param-
forward acknowledgments to the kernel to move head for-           eters, such as initial window size, rate, and credit.
ward and free up space for new data in the socket buffer.            We evaluate the following two aspects of Tonic:
   C and T can be configured for each flow based on its la-       Hardware Design (§6.1). We use Tonic’s Verilog prototype
tency and throughput characteristics. For high-bandwidth          to evaluate its hardware architecture for programmability and
flows, C can be set to MSS (or larger, if using TSO). For         scalability. Can Tonic support a wide range of transport pro-
flows that sporadically generate small segments, setting C        tocols? Does it reduce the development effort of implement-
and T is not as straightforward since segments cannot be          ing transport protocols in the NIC? Can Tonic support com-
consolidated within Tonic. We discuss the trade-offs in de-       plex user-defined logic with several variables? How many
ciding these parameters in detail in appendix D.                  per-flow segments and concurrent flows can it support?
Other Considerations. As we show in §6, Tonic’s current           End-to-End Behavior (§6.2). We use Tonic’s cycle-accurate

simulator and NS3 to compare Tonic’s end-to-end behavior                          User-Defined         Look up Tables (LUTs)
                                                                                               Credit                                      BRAMs
                                                                                     Logic            User-Defined    Fixed
with that of hard-coded implementations of two protocols:                         LoC
                                                                                               Type
                                                                                         state(B)          total(K)    % total(K)     % total    %
New Reno [23] and RoCEv2 with DCQCN [43], both for a                    Reno       48          8 wnd       2.4        0.5   109.4   20.9   195   20
single flow and for multiple flows sharing a bottleneck link.           NewReno    74         13 wnd       2.6        0.5   112.5   21.5   211   21
                                                                        SACK      193         19 wnd       3.3        0.6   112.1   21.4   219   22
6.1     Hardware Design                                                 NDP        20          1 token     3.0        0.6   143.6   29.0   300   30
                                                                        RoCE w/
                                                                                   63         30    rate   0.9        0.2 185.2 35.2 251         26
   There are two main metrics for evaluating the efficiency             DCQCN
                                                                        IRN        54         14    rate   2.9        0.6 177.4 33.9 219         22
of a hardware design: (i) Resource Utilization. FPGAs
consist of primitive blocks, which can be configured and                  Table 3: Resource utilization of transport protocols in Tonic.
connected to implement a Verilog program: look-up tables                   Reno [13] and New Reno [23] represent TCP variants that
(LUTs) are the main reconfigurable logic blocks, and block              use only cumulative acks for reliable delivery and congestion
RAMs (BRAMs) are used to implement memory. (ii) Tim-                    window for credit management. Reno can only recover from
ing. At the beginning of each cycle, each module’s input is             one loss within the window using fast retransmit, whereas
written to a set of input registers. The module must process            New Reno uses partial acknowledgments to recover more ef-
the input and prepare the result for the output registers before        ficiently from multiple losses in the same window. SACK,
the next cycle begins. Tonic must meet timing at 100 MHz to             inspired by RFC 6675 [16], represents TCP variants that use
transmit a segment address every 10 ns. That is, to achieve             selective acks. Our implementation has one SACK block
100 Gbps, the processing delay of every path from input to              per ack but can be extended to more. NDP [24] represents
output registers in every module must stay within 10 ns.                receiver-driven protocols, recently proposed for low-latency
   We use these two metrics to evaluate Tonic’s programma-              data-center networks [21, 36]. It uses explicit NACKs and
bility and scalability. These metrics are highly dependent on           timeouts for loss detection and grant tokens for congestion
the specific target used for synthesis. We use the Kintex Ul-           control. RoCEv2 w/ DCQCN [43] is a widely-used transport
trascale+ XCKU15P FPGA as our target because this FPGA,                 for RDMA over Ethernet, and IRN [34] is a recent hardware-
and others with similar capabilities, are included as bump-             based protocol for improving RoCE’s simple data delivery
in-the-wire entities in today’s commercial programmable                 algorithm. Both use rate limiters for credit management.
NICs [2, 3]. This is a conservative choice, as these NICs                  Note that, as described in §3.2, not all data delivery al-
are designed for 10-40 Gbps Ethernet. A 100 Gbps NIC                    gorithms are feasible for hardware implementation as is. For
could potentially have a more powerful FPGA. Moreover,                  instance, due to memory constraints on the NIC, it is not pos-
we synthesize all of Tonic’s components onto the FPGA                   sible to keep timestamps for every packet, new and retrans-
to evaluate it as a standalone prototype. However, given                missions, on the NIC. As a result, transport protocols which
the well-defined interfaces between the fixed-function and              rely heavily on per-packet timestamps, e.g., QUIC [27], need
programmable modules, it is conceivable to implement the                to be modified to work with fewer timestamps, i.e., for a sub-
fixed-function components as an ASIC for more efficiency.               set of in-flight segments, to be offloaded to hardware.
Unless stated otherwise, we set the maximum number of                   Takeways. There are three key takeaways from these results:
concurrent flows to 1024 and the maximum window size to
                                                                        • Tonic supports a variety of transport protocols.
128 segments in all of our experiments 7 .
                                                                        • Tonic enables programmers to implement new transport
6.1.1   Hardware Programmability                                           logic with modest development effort. Using Tonic, each
                                                                           of the above protocols is implemented in less than 200
   We have implemented the sender’s transport logic of six
                                                                           lines of Verilog code, with the user-defined logic con-
protocols in Tonic as representatives of various types of seg-
                                                                           suming less than 0.6% of the FPGA’s LUTs. In contrast,
ment selection and credit calculation algorithms in the lit-
                                                                           Tonic’s fixed-function modules, which are reused across
erature. Table 3 summarizes their resource utilization for
                                                                           these protocols, are implemented in ∼8K lines of code and
both fixed-function and user-defined modules, and the lines
                                                                           consume ∼60 times more LUTs.
of code and bytes of user-defined state used to implement
them. While we use the same set of per-flow state variables             • Different credit management schemes have different over-
(Table 2) for all protocols, not all of them use all the vari-             heads. For transport protocols that use congestion win-
ables in processing transport events. For instance, bitmaps                dow, window calculations overlap with and therefore are
are only used by protocols with selective acks. Thus, it is                implemented in the data delivery engine (§3.3.1). Thus,
possible to reduce the resource utilization even more with                 their credit engine utilizes fewer resources than others.
some pre-processing to remove the irrelevant variables and                 Rate limiting requires more per-flow state and more com-
computation from the Verilog design.                                       plicated operations (§4) than enforcing receiver-generated
                                                                           grant tokens but needs fewer memory ports for concurrent
   7A   100 Gbps flow with 1500B back-to-back packets over 15-µs RTT,      reads and writes (§3.3.2), overall leading to lower BRAM
typical in data centers, has at most 128 in-flight segments.               and higher LUT utilization for rate limiting.

                    140                                                         5                                                             1.0
                                              Tonic                                       Tonic                                                        Tonic




                                                               Transmitted Sequence
                    120
Congestion Window


                                              Hard-Coded                        4         Hard-Coded                                          0.8      Hard-Coded




                                                                  Number ( ×106 )
                    100
    (KiloBytes)




                     80                                                         3                                                             0.6




                                                                                                                                        CDF
                     60                                                         2                                                             0.4
                     40
                                                                                1                                                             0.2
                     20
                      00                                                        0                                                             0.00
                              1    2      3      4         5                          0    1      2    3         4      5                              20     40    60    80     100
                              Time (milliseconds)                                          Time (milliseconds)                                       Average Throughput (Mbps)
                                      (a)                                                          (b)                                                           (c)
   Figure 3: NewReno’s Tonic vs hard-coded implementation in NS3 (10G line-rate): a) Congestion window updates (single flow, random
   drops), b) Transmitted sequence numbers with retransmission in large dots (single flow, random drops), and c) CDF of average throughput of
   multiple flows sharing a bottleneck link over 5 seconds (200 flows from 2 hosts to one receiver)

   6.1.2                   Hardware Scalability                                                                                      Metric  Results
                                                                                                                                             ( 0 , 31]      meets timing
                                                                                                             Complexity of      logic
      We evaluate Tonic’s scalability by examining how sources                                                                               (31, 42]       depends on operations
                                                                                                             User-Defined Logic levels
                                                                                                                                             (42, 65]       violates timing
   of variability in its architecture (programmable modules and
                                                                                                                                             256            grant token
   various parameters) affect memory utilization and timing.                                                 User-Defined State     bytes    340            rate
   User-defined logic in programmable modules can have                                                                                       448            congestion window
   arbitrarily-long chains of dependent operations, potentially                                              Window Size            segments 256
                                                                                                             Concurrent Flows       count    2048
   causing timing violations. We generate 70 random programs
   for incoming.v (the programmable stage of Incoming mod-                                                            Table 4: Summary of Tonic’s scalability results.
   ule in data delivery engine) with different numbers of arith-
   metic, logical, and bitmap operations, and analyze how long                                         utilization, and the complexity of bitmap operations, hence
   the chain of dependent operations gets without violating tim-                                       timing. Tonic can support bitmaps as large as 256 bits (i.e.,
   ing at 10ns. These programs use up to 125B of state and                                             tracking 256 segments), with which we can support a single
   have a maximum dependency of 65 logic levels (respectively                                          100Gbps flow in a network with up to 30µs RTT (Table 4).
   six and two times more than the benchmark protocols in Ta-                                             Maximum number of concurrent flows determines
   ble 3). Each logic level represents one of several primitive                                        memory depth and the size of FIFOs used for flow schedul-
   logic blocks (LUT, MUX, DSP, etc.) chained together to im-                                          ing (§3.1). Thus, it affects both memory utilization and the
   plement a path in a Verilog program.                                                                queue operations, hence timing. Tonic can scale to 2048 con-
      We plug these programs into Tonic, synthesize them, and                                          current flows in hardware (Table 4) which matches the size
   analyze the relationship between the number of logic levels                                         of the active flow set observed in data centers [15, 37] and
   and latency of the max-delay path (Table 4). Programs with                                          other hardware offloads in the literature [20].
   up to 32 logic levels consistently meet timing, while those                                            Takeways. Tonic has additional room to support future
   with more than 43 logic levels do not. Between 32 and 42                                            protocols that are more sophisticated with more user-defined
   logic levels, the latency of the max-delay path is around 10                                        variables than our benchmark protocols. It can track 256
   ns. Depending on the mix of primitives on the max-delay                                             segments per flow and support 2048 concurrent flows. With
   path and their latencies, programs in that region can poten-                                        a more powerful FPGA with more BRAMs, Tonic can po-
   tially meet timing. Our benchmark protocols have 13 to 29                                           tentially support even larger windows and more flows.
   logic levels on their max-delay path and all meet timing.
   Thus, Tonic not only supports our benchmark protocols, but                                          6.2           End-to-End Behavior
   also has room to support future more sophisticated protocols.                                          To examine Tonic’s end-to-end behavior and verify the fi-
   User-defined state variables increase the memory width af-                                          delity of Tonic-based implementation of the transport logic
   fecting BRAM utilization. We add extra variables to SACK,                                           in different protocols, we have developed a cycle-accurate
   IRN, and NDP to see how wide memories can get without                                               hardware simulator for Tonic in C++. We use this sim-
   violating timing and running out of BRAMs, repeating the                                            ulator with NS3 to show that Tonic-based implementation
   experiment for each of the three credit management schemes                                          of NewReno and RoCEv2 w/ DCQCN senders match their
   as they have different memory footprints (Table 4). Tonic                                           hard-coded NS3 implementation. Note that the goal of these
   can support 448B of user-defined state with congestion win-                                         simulations is to analyze and verify Tonic’s end-to-end be-
   dow for credit management, 340B with rate, and 256B with                                            havior. Tonic’s ability to support 100Gbps line rate has al-
   grant tokens (Protocols in Table 3 use less than 30B).                                              ready been demonstrated in §6.1 using hardware synthesis.
   Maximum window size determines the size of per-flow                                                 Thus, in our simulations, we use 10Gbps and 40Gbps as
   bitmaps stored in the data delivery engine to keep track of                                         line rate merely to make hardware simulations with multi-
   the status of a flow’s segments, therefore affecting memory                                         ple flows over seconds computationally tractable.

                                  40                                         trol as opposed to packet loss in TCP. Thus, to observe rate
                                  35                        Tonic            updates for a single flow, we run two flows from two hosts
                                                            Hard-Coded

              Throughput (Gpbs)
                                  30                                         to the same receiver for one second to create congestion and
                                  25
                                                                             track the throughput changes of one as they both converge
                                  20
                                  15                                         to the same rate. Tonic’s behavior closely matches the hard-
                                  10                                         coded implementation (Figure 4). We also ran a single DC-
                                   5                                         QCN flow at 100Gbps with 128B back-to-back packets and
                                   00   1        2      3         4      5   confirmed that Tonic can saturate the 100Gbps link.
                                            Time (milliseconds)
                                                                             Multiple Flows. Two senders each start 100 flows to a single
Figure 4: RoCEv2 w/ DCQCN in Tonic vs hard-coded in NS3 (40G                 receiver, so 200 flows share a single bottleneck link for one
line rate, one of two flows on a bottleneck link).
                                                                             second. Both Tonic and the hard-coded implementation do
                                                                             per-packet round robin scheduling among the flows on the
6.2.1   TCP New Reno
                                                                             same host. As a result, all flows in both cases end up with
   We implement TCP New Reno in Tonic based on                               an average throughput of 203 ± 0.2Mbps. Moreover, we ob-
RFC 6582, and use NS3’s native network stack for its hard-                   serve a matching distribution of CNPs in both cases.
coded implementation. Our Tonic-based implementation
works with the unmodified native TCP receiver in NS3. In                     7   Related Work
all simulations, hosts are connected via 10Gbps links to one
                                                                                Tonic is the first programmable architecture for transport
switch, RTT is 10µs, the buffer is 5.5MB, the minimum re-
                                                                             logic in hardware able to support 100 Gbps. In this section,
transmission timeout is 200ms (Linux default), segments are
                                                                             we review the most closely related prior work.
1000B large, and delayed acks are enabled on the receiver.
                                                                                Commercial hardware network stacks. Some NICs
Single Flow. We start a single flow from one host to an-
                                                                             have hardware network stacks with hard-wired transport pro-
other, and randomly drop packets on the receiver’s NIC. Fig-
                                                                             tocols [8, 10]. However, they only implement two proto-
ure 3(a) and 3(b) show the updates to the congestion win-
                                                                             cols, either RoCE [8] or a vendor-selected TCP variant, and
dow and transmitted sequence numbers (retransmissions are
                                                                             can only be modified by their vendor. Tonic enables pro-
marked with large dots), respectively. Tonic’s behavior in
                                                                             grammers to implement a variety of transport protocols in
both cases closely matches the hard-coded implementation.
                                                                             hardware with modest effort. In the absence of a publicly-
The slight differences stem from the fact that in NS3’s net-
                                                                             available detailed description of these NICs’ architecture, we
work stack, all the computation happens in the same virtual
                                                                             could not compare our design decisions with theirs.
time step while in Tonic every event (incoming packets , seg-
ment address generation, etc.) is processed over a 100ns cy-                    Non-commercial hardware transport protocols. Recent
cle (increased from 10ns to match the 10G line rate).                        work explores hardware transport protocols that run at high
                                                                             speed with low memory footprint [30, 31, 34]. Tonic facil-
Multiple Flows. Two senders each start 100 flows to a single
                                                                             itates innovation in this area by enabling researchers to im-
receiver, so 200 flows share a single bottleneck link for 5 sec-
                                                                             plement new protocols with modest development effort.
onds. The CDF of average throughput across the 200 flows
                                                                                Accelerating network functionality. Several academic
for the Tonic-based implementation closely matches that of
                                                                             and industrial projects offload end-host virtual switching and
the hard-coded implementation (Figure 3(c)). We observe
                                                                             network functions to FPGAs, processing a stream of already-
similarly matching distributions for number of retransmis-
                                                                             generated packets [14, 20, 28, 29, 41]. Tonic, on the other
sions. When analyzing the flows’ throughput in millisecond-
                                                                             hand, implements the transport logic in the NIC by keeping
long epochs, we notice larger variations in the hard-coded
                                                                             track of potentially a few hundred segments at a time to gen-
implementation than Tonic since Tonic, as opposed to NS3’s
                                                                             erate packets at line rate while running user-defined transport
stack, performs per-packet round robin scheduling across
                                                                             logic to ensure efficient and reliable delivery.
flows on the same host.
6.2.2   RoCEv2 with DCQCN                                                    Acknowledgments
   We implement RoCE w/ DCQCN [43] in Tonic, and use                            We thank Aditya Akella, our shepherd, the anonymous re-
the authors’ NS3 implementation from [44] for the hard-                      viewers of NSDI’20, Hari Balakrishnan, Gordon Bebner, and
coded implementation. Our Tonic-based implementation                         Radhika Mittal for their helpful feedback. We thank Behnaz
works with the unmodified hard-coded RoCE receiver. In                       Arzani, Xiaoqi Chen, Rob Harrison, Suriya Kodeswaran,
all simulations, hosts are connected via 40Gbps links to the                 Shir Landau-Feibish, Srinivas Narayana, Praveen Tammana,
same switch, RTT is 4µs, segments are 1000B large, and we                    and Ross Teixeira for feedback on earlier drafts of this work.
use the default DCQCN parameters from [44].                                  This work is supported by DARPA under Dispersed Com-
Single Flow. DCQCN is a rate-based algorithm which uses                      puting HR0011-17-C-0047, NSF Grants No. CCF-1822949
CNPs and periodic timers and counters for congestion con-                    and CCF-1453112, and DARPA No. FA8650-18-2-7846.

References                                                                     [23] H ANDERSON , T., F LOYD , S., G URTOV, A., AND N ISHIDA , Y. The
                                                                                    NewReno Modification to TCP’s Fast Recovery Algorithm. RFC
 [1] F-Stack. http://www.f-stack.org/. Accessed: August 2019.                       6582, 1999.
 [2] Innova Flex 4 Lx EN Adapter Card. http://www.mellanox.com/                [24] H ANDLEY, M., R AICIU , C., AGACHE , A., VOINESCU , A., M OORE ,
     related-docs/prod_adapter_cards/PB_Innova_Flex4_Lx_                            A. W., A NTICHI , G., AND W ÓJCIK , M. Re-architecting Datacenter
     EN.pdf. Accessed: August 2019.                                                 Networks and Stacks for Low Latency and High Performance. In SIG-
                                                                                    COMM (2017).
 [3] Mellanox Innova 2 Flex Open Programmable SmartNIC.
     http://www.mellanox.com/related-docs/prod_adapter_                        [25] J EONG , E., W OO , S., JAMSHED , M. A., J EONG , H., I HM , S., H AN ,
     cards/PB_Innova-2_Flex.pdf. Accessed: August 2019.                             D., AND PARK , K. mTCP: a Highly Scalable User-level TCP Stack
                                                                                    for Multicore Systems. In NSDI (2014).
 [4] NS3 Network Simulator. https://www.nsnam.org/. Accessed:
     August 2019.                                                              [26] L A F OREST, C. E., AND S TEFFAN , J. G. Efficient Multi-Ported
                                                                                    Memories for FPGAs. In FPGA (2010).
 [5] NVMe over Fabric. https://nvmexpress.org/wp-content/
     uploads/NVMe_Over_Fabrics.pdf. Accessed: August 2019.                     [27] L ANGLEY, A., R IDDOCH , A., W ILK , A., V ICENTE , A., K RASIC ,
                                                                                    C., Z HANG , D., YANG , F., KOURANOV, F., S WETT, I., I YENGAR ,
 [6] OpenOnload. https://www.openonload.org/. Accessed: August                      J., ET AL . The QUIC Transport Protocol: Design and Internet-Scale
     2019.                                                                          Deployment. In SIGCOMM (2017).
 [7] RDMA - iWARP.              https://www.chelsio.com/nic/                   [28] L AVASANI , M., D ENNISON , L., AND C HIOU , D. Compiling High
     rdma-iwarp/. Accessed: August 2019.                                            Throughput Network Processors. In FPGA (2012).
 [8] RDMA and RoCE for Ethernet Network Efficiency Performance.                [29] L I , B., TAN , K., L UO , L. L., P ENG , Y., L UO , R., X U , N., X IONG ,
     http://www.mellanox.com/page/products_dyn?product_                             Y., C HENG , P., AND C HEN , E. Clicknp: Highly Flexible and High
     family=79&mtag=roce. Accessed: August 2019.                                    Performance Network Processing with Reconfigurable Hardware. In
                                                                                    SIGCOMM (2016).
 [9] RoCE Accelerates Data Center Performance, Cost Efficiency, and
     Scalability. http://www.roceinitiative.org/wp-content/                    [30] L U , Y., C HEN , G., L I , B., TAN , K., X IONG , Y., C HENG , P.,
     uploads/2017/01/RoCE-Accelerates-DC-performance_                               Z HANG , J., C HEN , E., AND M OSCIBRODA , T. Multi-Path Trans-
     Final.pdf. Accessed: August 2019.                                              port for RDMA in Datacenters. In NSDI (2018).
[10] TCP Offload Engine (TOE). https://www.chelsio.com/nic/                    [31] L U , Y., C HEN , G., RUAN , Z., X IAO , W., L I , B., Z HANG , J.,
     tcp-offload-engine/. Accessed: August 2019.                                    X IONG , Y., C HENG , P., AND C HEN , E. Memory Efficient Loss Re-
                                                                                    covery for Hardware-Based Transport in Datacenter. In Asia-Pacific
[11] Tonic Github Repository. https://github.com/minmit/tonic.                      Workshop on Networking (2017).
[12] A LIZADEH , M., G REENBERG , A., M ALTZ , D. A., PADHYE , J., PA -        [32] M ARINOS , I., WATSON , R. N., AND H ANDLEY, M. Network Stack
     TEL , P., P RABHAKAR , B., S ENGUPTA , S., AND S RIDHARAN , M.                 Specialization for Performance. In SIGCOMM (2014).
     Data Center TCP (DCTCP). In SIGCOMM (2010).
                                                                               [33] M ATHIS , M., AND M AHDAVI , J. Forward Acknowledgement: Refin-
[13] A LLMAN , M., PAXSON , V., AND B LANTON , E. TCP Congestion                    ing TCP Congestion Control. In SIGCOMM (1996).
     Control. RFC 5681, 2009.
                                                                               [34] M ITTAL , R., S HPINER , A., PANDA , A., Z AHAVI , E., K RISHNA -
[14] A RASHLOO , M. T., G HOBADI , M., R EXFORD , J., AND WALKER ,                  MURTHY, A., R ATNASAMY, S., AND S HENKER , S. Revisiting Net-
     D. HotCocoa: Hardware congestion control abstractions. In HotNets              work Support for RDMA. In SIGCOMM (2018).
     (2017).
                                                                               [35] M ITTAL , R., T HE L AM , V., D UKKIPATI , N., B LEM , E., WASSEL ,
[15] B ENSON , T., A KELLA , A., AND M ALTZ , D. A. Network Traffic                 H., G HOBADI , M., VAHDAT, A., WANG , Y., W ETHERALL , D., AND
     Characteristics of Data Centers in the Wild. In IMC (2010).                    Z ATS , D. TIMELY: RTT-Based Congestion Control for the Datacen-
                                                                                    ter. In SIGCOMM (2015).
[16] B LANTON , E., A LLMAN , M., WANG , L., JARVINEN , I., KOJO , M.,
     AND N ISHIDA , Y. A Conservative Loss Recovery Algorithm Based            [36] M ONTAZERI , B., L I , Y., A LIZADEH , M., AND O USTERHOUT, J.
     on Selective Acknowledgment (SACK) for TCP. RFC 6675, 2012.                    Homa: A Receiver-Driven Low-Latency Transport Protocol Using
                                                                                    Network Priorities. In SIGCOMM (2018).
[17] C ARDWELL , N., C HENG , Y., G UNN , C. S., Y EGANEH , S. H., AND
     JACOBSON , V. BBR: Congestion-Based Congestion Control. ACM               [37] ROY, A., Z ENG , H., BAGGA , J., P ORTER , G., AND S NOEREN , A. C.
     Queue (2016).                                                                  Inside the Social Network’s (Datacenter) Network. In SIGCOMM
                                                                                    (2015).
[18] C ONNERY, G. W., S HERER , W. P., JASZEWSKI , G., AND B INDER ,
     J. S. Offload of TCP Segmentation to a Smart Adapter, 1999. US            [38] S AEED , A., D UKKIPATI , N., VALANCIUS , V., C ONTAVALLI , C.,
     Patent 5,937,169.                                                              AND VAHDAT, A. Carousel: Scalable Traffic Shaping at End Hosts.
                                                                                    In SIGCOMM (2017).
[19] D ONG , M., L I , Q., Z ARCHY, D., G ODFREY, P. B., AND S CHAPIRA ,
     M. PCC: Re-architecting Congestion Control for Consistent High Per-       [39] S HREEDHAR , M., AND VARGHESE , G. Efficient Fair Queuing Us-
     formance. In NSDI (2015).                                                      ing Deficit Round-Robin. IEEE/ACM Transactions on Networking
                                                                                    (1996).
[20] F IRESTONE , D., ET AL . Azure Accelerated Networking: SmartNICs
                                                                               [40] S IVARAMAN , A., S UBRAMANIAN , S., A LIZADEH , M., C HOLE , S.,
     in the Public Cloud. In NSDI (2018).
                                                                                    C HUANG , S.-T., AGRAWAL , A., BALAKRISHNAN , H., E DSALL , T.,
[21] G AO , P. X., NARAYAN , A., K UMAR , G., AGARWAL , R., R AT-                   K ATTI , S., AND M C K EOWN , N. Programmable Packet Scheduling
     NASAMY, S., AND S HENKER , S. pHost: Distributed Near-Optimal                  at Line Rate. In SIGCOMM (2016).
     Datacenter Transport Over Commodity Network Fabric. In CoNEXT
                                                                               [41] S ULTANA , N., G ALEA , S., G REAVES , D., W ÓJCIK , M., S HIPTON ,
     (2015).
                                                                                    J., C LEGG , R., M AI , L., B RESSANA , P., S OUL É , R., M ORTIER ,
[22] G UO , C., W U , H., D ENG , Z., S ONI , G., Y E , J., PADHYE , J., AND        R., C OSTA , P., P IETZUCH , P., C ROWCROFT, J., M OORE , A., AND
     L IPSHTEYN , M. RDMA over Commodity Ethernet at Scale. In SIG-                 Z ILBERMAN , N. Emu: Rapid Prototyping of Networking Services.
     COMM (2016).                                                                   In ATC (2017).

[42] W ILSON , C., BALLANI , H., K ARAGIANNIS , T., AND ROWTRON , A.     by whichever transport protocol one chooses to implement
     Better Never than Late: Meeting Deadlines in Datacenter Networks.   on Tonic on the sender side.
     In SIGCOMM (2011).
                                                                            While some implementations of RDMA over Ethernet
[43] Z HU , Y., E RAN , H., F IRESTONE , D., G UO , C., L IPSHTEYN ,
     M., L IRON , Y., PADHYE , J., R AINDEL , S., YAHIA , M. H., AND
                                                                         such as iWarp [7] handle out-of-order (OOO) packets and
     Z HANG , M. Congestion Control for Large-Scale RDMA Deploy-         implement TCP/IP-like acknowledgments, others, namely
     ments. In SIGCOMM (2015).                                           RoCE [8], assume a lossless network and have simpler trans-
[44] Z HU , Y., G HOBADI , M., M ISRA , V., AND PADHYE , J. ECN or       port protocols that do not require receivers to handle OOO
     Delay: Lessons Learnt from Analysis of DCQCN and TIMELY. In         packets and generate frequent control signals. However, as
     CoNEXT (2016).                                                      RDMA over Ethernet is getting more common in data cen-
                                                                         ters, the capability to handle OOO packets on the receiver
A     Integrating Tonic within RDMA                                      and generate various control signals for more efficient trans-
   Remote Direct Memory Access (RDMA) enables applica-                   port is being implemented in these NICs as well [34, 43].
tions to directly access memory on remote endpoints without                 Finally, Tonic provides in-order reliable data delivery
involving the CPU. To do so, the endpoints create a queue                within the same flow. Thus, messages sent over the same
pair, analogous to a connection, and post requests, called               flow are delivered to the receiver in the same order. How-
Work Queue Elements (WQEs), for sending or receiving data                ever, it is sometimes desirable to support out-of-order mes-
from each other’s memory. While RDMA originated from                     sage delivery for a communication endpoint (e.g., a queue
InfiniBand networks, RDMA over Ethernet is getting more                  pair), for instance, to increase the performance of applica-
common in data centers [9, 22, 35]. In this section, we use              tions when messages are independent from each other, or
RDMA to refer to RDMA implementations over Ethernet.                     when using “unconnected” endpoints (e.g., one sender and
   Once a queue pair is created, RDMA NICs can add the                   multiple receivers). It is still possible to support out-of-order
new “connection” to Tonic and use it to on the sender side to            message delivery using Tonic by creating multiple flows in
transfer data in response to different WQEs. Each WQE cor-               Tonic for the same communication endpoint and using them
responds to a separate message transfer and therefore nicely             concurrently. Extending Tonic to support out-of-order mes-
fits Tonic’s need for determining segment boundaries before              sage delivery within the same flow is an interesting avenue
starting data transmission.                                              for future work.
   For instance, in an RDMA Write, one endpoint posts a
Request WQE to write to the memory on the other endpoint.                B    High-Precision Per-Flow Rate Limiting
Data length, data source address on the sender, and data sink
addresses on the receiver are specified in the Request WQE.                 When using rate in the credit engine, if a flow with rate
Thus, a shim layer between RDMA applications and Tonic                   R bytes per cycle needs L more bytes of credit to transmit
can decide the segment boundaries and notify Tonic of the                a segment, Tonic calculates T = d RL e as the time where the
number of segments and the source memory address to read                 flow will have sufficient credit for transmission. It sets up
the data from on the sender. Once Tonic generates the next               a timer that expires in T cycles, and upon its expiry, queues
segment address, the rest of the RDMA NIC can DMA it                     up the flow in ready-to-tx for transmission (§3.3.2). Since
from the sender’s memory and add appropriate headers. An                 Tonic cannot afford to do floating-point division within its
RDMA Send is similar to RDMA Write, except it requires a                 timing constraints, R must be represented as an integer.
Receive WQE on the receiver to specify the sink address to                  This creates a trade-off between the rate-limiting precision
which the data from the sender should be written. So, Tonic              and the range of rates Tonic can support. If we represent R
can still be used in the same way on the sender side.                    in bytes per cycle, we can compute the exact cycle when the
   As another example, in an RDMA Read, one endpoint re-                 flow will have enough credit, but cannot support rates lower
quests data from the memory on the other endpoint. So, the               than one byte per cycle or ∼1 Gbps. If we instead represent
responder endpoint should transmit data to the requester end-            R in, say, bytes per thousand cycles, we can support lower
point. Again, the data length, data source address on the re-            rates (e.g., 1 Mbps), but T = d RL e will determine how many
sponder, and data sink address on the requester are specified            thousand cycles from now the flow can qualify for transmis-
in the WQE. Thus, the shim layer can decide the segment                  sion. This results in lower rate conformance and precision
boundaries and and transfer the data using Tonic.                        for higher-bandwidth flows. As a concrete example, for a
   Thus, Tonic can be integrated into RDMA NICs to re-                   20 Gbps flow, R would be 25000 bytes per thousand cycles.
place the hard-coded transport logic on the sender-side of               Suppose the flow has a 1500-byte segment to transmit. It
data transfer. In fact, two of our benchmark protocols, RoCE             will have enough credit to do so in 8 cycles but has to wait
                                                                           1500
w/ DCQCN [43] and IRN [34], are proposed for RDMA                        d 25000 e = 1 thousand cycles to be queued for transmission.
NICs. That said, this is assuming there is a compatible re-                 Instead of committing to one representation for R, Tonic
ceiver on the other side to generate the control signals (e.g.,          keeps multiple variables R1 , . . . , Rk for each flow, each rep-
acknowledgments, congestion notifications, etc.) required                resenting flow’s rate at a different level of precision. As the

congestion control loop adjusts the rate according to network         C, which is the flow’s fixed segment size, and (ii) T , which
capacity, Tonic can efficiently switch between R1 , . . . , Rk to     is the duration that the Kernel waits for more data from the
pick the most precise representation for computing T at any           application before sending a “sub-segment” (a segment that
moment. This enables Tonic to support a wide range of rates           is smaller than C) to Tonic. C and T can be configured for
without sacrificing the rate-limiting precision.                      each flow based on its latency and throughput characteristics.
                                                                      For high-bandwidth flows, C can be set to MSS (or larger, if
C    Efficient Bitmap Operations                                      using TSO). For flows that only sporadically generate data
                                                                      segments, setting C and T , as we discuss below, is not as
    Tonic uses bitmaps as large as 128 bits to track the status
                                                                      straightforward.
of a window of segments for each flow. Bitmaps are im-
                                                                         With a fixed C, increasing T results in more small seg-
plemented as ring buffers, with the head pointer correspond-
                                                                      ments being consolidated into C-sized segments before being
ing to the first unacknowledged segment. As new acknowl-
                                                                      sent to Tonic for transmission, but at the expense of higher
edgments arrive, the head pointer moves forward around the
                                                                      latency. C determines the size of the segments and number of
ring. To efficiently implement operations whose output de-
                                                                      sub-segments generated by Tonic. Recall from §5 that a sub-
pends on the values of all the bits in the bitmap, we must par-
                                                                      segment is created when there is not enough data to make
allelize them by dividing the ring buffer into smaller parts,
                                                                      a full C-sized segment within T . Tonic needs all segments,
processing them in parallel, and joining the results. For large
                                                                      except for the last sub-segment in a burst, to be of equal size.
ring buffers, this divide and conquer pattern is repeated in
                                                                      Thus, even if more data is added to the socket buffer after the
multiple layers. As each layer depends on the previous one
                                                                      sub-segment is sent to Tonic for transmission, Tonic has to
for its input, we must keep the computation in each layer
                                                                      successfully deliver all the previous segments before it can
minimal to stay within our 10 ns target.
                                                                      start transmitting the new ones. Thus, it is desirable to pro-
    One such operation finds the first set bit after the head.        duce larger segments but fewer sub-segments. With a fixed
This operation is used to find the next lost segment for re-          T , increasing C results in larger segments. However, to pro-
transmission in the marked-for-rtx bitmap. The moving                 duce fewer sub-segments, C should be picked such that in
head of the ring buffer complicates the implementation of             most cases, the data within a burst is divisible by C. Bursts
this operation. Suppose we have a 32-bit ring buffer A32 ,            are separated in time by T . So the choice of T affects the
with bits 5 and 30 set to one, and the head at index 6. Thus,         choice of C and vice versa.
 f ind f irst(A32 , 6) = 30. We divide the ring into eight four-         For instance, if an application periodically generates 128-
bit parts, “or” the bits in each one, and feed the results into       byte requests, C can be easily set to 128 and T based on the
an 8-bit ring buffer A8 , where A8 [i] = OR(A32 [i : i + 3]). So,     periodicity. As another example, for an application that peri-
only A8 [1] and A8 [7] are set. However, because the set bit          odically generates segments of widely-varying sizes, setting
in A32 [4 : 7] is before the head in the original ring buffer, we     T to zero and C to the maximum expected segment size re-
cannot simply use one as A8 ’s head index or we will mistak-          sults in Tonic transmitting data segments as generated by the
enly generate 5 instead of 30 as the final result. So, we need        application without consolidation, potentially creating many
extra computation to find the correct new head. For a larger          sub-segments. For the same application, setting T to zero
ring buffer with multiple layers of this divide and conquer           and C to the minimum expected segment size could result in
pattern, we need to compute the head in each layer.                   Tonic generating many small segments as all segments will
    Instead, we use a lightweight pre-processing on the in-           be broken into the minimum expected segment size. Note
put ring buffer to avoid head index computation altogether.           that these trade-offs become more pronounced if Tonic is to
More specifically, using A32 as input, we compute A032 which          be used for flows that only sporadically generate data seg-
is equal to A32 except that all the bits from index zero to           ments. For high-bandwidth flows, C can be set to MSS (or
head (6 in our example) are set to zero. Starting from in-            larger, if using TSO), and T depending on the application’s
dex zero, the first set bit in A032 is always closer to the orig-     traffic pattern and burstiness.
inal head than the first set bit in A32 . So, f ind f irst(A32 , 6)
equals f ind f irst(A032 , 0) if A032 has any set bits, and other-    E    Government Disclaimer
wise f ind f irst(A32 , 0). This way, independent of the input
head index H, we can always solve f ind f irst(A, H) from                The U.S. Government is authorized to reproduce and dis-
two subproblems with the head index fixed at zero.                    tribute reprints for Governmental purposes notwithstanding
                                                                      any copyright notation thereon. The views and conclusions
D    Deciding C and T for Flows Using Tonic                           contained herein are those of the authors and should not be
     through the Socket API                                           interpreted as necessarily representing the official policies or
                                                                      endorsements, either expressed or implied, of Air Force Re-
   In §5, we provide an example of how Tonic can be inte-             search Laboratory (AFRL) and Defense Advanced Research
grated into the Linux Kernel so that applications can use it          Projects Agency (DARPA), NSF, or the U.S. Government.
through the Socket API. We introduce two parameters: (i)

     F        New Reno in Tonic
       The following is the implementation of New Reno’s loss detection and recovery algorithm on receipt of acknowledgments in
     Tonic [23]. Extra comments have been added for clarification.
 1   module ne w_r eno _i n c o m i n g (
 2     /* * * * * * * * * * * * * * * * * * * * * * * * * INPUTS * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
 3     // ACK , NACK , SACK , CNP , etc ...
 4     input          [ ‘PKT_TYPE_W -1:0 ]                     pkt_type ,
 5     input          [ ‘PKT_DATA_W -1:0 ]                     pkt_data_in ,
 6
 7        // Segment ID in the cumulative ack nowledg ment
 8        input   [ ‘SEGMENT_ID_W -1:0 ]  cumulative_ack ,
 9
10        // Segment ID that is selectively acknowledged , if any
11        input   [ ‘SEGMENT_ID_W -1:0 ]  selective_ack ,
12
13        // Number of segments acknowledged with the received ac knowled gment
14        input   [ ‘WINDOW_INDEX_W -1:0 ] newly_acked_cnt ,
15
16        // Segment ID at the beginning of the window , before and after the
17        // acknowledgment
18        input   [ ‘WINDOW_INDEX_W -1:0 ] old_wnd_start ,
19        input   [ ‘WINDOW_INDEX_W -1:0 ] new_wnd_start ,
20
21        // Current time in nanoseconds
22        input   [ ‘TIME_W -1:0 ]                                      now ,
23
24        // // Per - Flow State
25
26        input         [ ‘MAX_WINDOW_SIZE -1:0 ]                       acked ,
27        input         [ ‘MAX_TX_CNT_SIZE -1:0 ]                       tx_cnt ,
28        input         [ ‘SEGMENT_ID_W -1:0 ]                          highest_sent ,
29        input         [ ‘SEGMENT_ID_W -1:0 ]                          wnd_start ,
30        input         [ ‘WINDOW_SIZE_W -1:0 ]                         wnd_size_in ,
31        input         [ ‘TIEMR_W -1:0 ]                               rtx_timer_amount_in ,
32        input         [ ‘SEGMENT_ID_W -1:0 ]                          total_tx_cnt ,
33
34        input         [ ‘USER_VARS_W -1:0 ]                           user_vars_in ,
35
36        /* * * * * * * * * * * * * * * * * * * * * * * * * OUTPUTS * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
37        output [ ‘FLAG_W -1:0 ]                                 mark_any_for_rtx ,
38        output [ ‘SEGMENT_ID_W -1:0 ]                           mark_for_rtx_from ,
39        output [ ‘SEGMENT_ID_W -1:0 ]                           mark_for_rtx_to ,
40        output [ ‘WINDOW_SIZE_W -1:0 ]                          wnd_size_out ,
41        output [ ‘TIMER_W -1:0 ]                                rtx_timer_amount_out ,
42        output [ ‘FLAG_W -1:0 ]                                 reset_rtx_timer ,
43
44        output        [ ‘USER_VARS_W -1:0 ]                           user_vars_out
45   );
46
47   /* * * * * * * * * * * * * * * * * * * * * * * * * Local Variables * * * * * * * * * * * * * * * * * * * *
48   *
49   * Declarations ommited for brevity
50   *
51   * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * * */
52
53   // / is the ack new or duplicate ?
54   assign is_dup_ack    = old_wnd_start == cumula tive_ack ;
55   assign is_new_ack    = new_wnd_start > old_wnd_start ;
56
57   // / count duplicated acks
58   assign dup_acks      = is_new_ack ? 0 :
59                          is_dup_ack ? dup_acks_in + 1 : dup_acks_in ;
60
61   // How many in_flight packets ?
62   assign sent_out     = highest_sent - wnd_start ;
63   assign in_flight    = sent_out - dup_acks ;
64
65   // update previous highest ack
66   assign p r e v _ h i g h e s t _ a c k _ o u t = is_new_ack ? old_wnd_start : p r e v _ h i g h e s t _ a c k _ i n ;
67
68   // / Should we do fast rtx ?
69   assign do_fast_rtx = dup_acks == ‘ D U P _ AC K S _ T H R E S H &
70                         (( cumul ative_ac k > recover_in ) |
71                          ( wnd_size_in > 1 & c umulativ e_ack - p r e v _ h i g h e s t _ a c k _ i n <= 4));
72
73   // if yes , update recovery sequence and updated ssh_thresh
74   assign recovery_se q _ o u t = do_fast_rtx ? highest_sent : re co v er y_ se q _i n ;
75

 76   assign half_wnd                   = in_flight > 2 ? in_flight >> 1 : 2;
 77   assign ss_thresh_out              = do_fast_rtx ? half_wnd : ss_thresh_in ;
 78
 79   // // if in recovery and this is a new ack , is it a
 80      // full ack or a partial ack ? ( Definition in RFC )
 81   assign full_ack = is_new_ack & c umulativ e_ack > recover_in ;
 82   assign partial_ack = is_new_ack & cumu lative_ ack <= recover_in ;
 83
 84   // mark for retra nsmissio n
 85   assign mark_any_fo r _ r t x = do_fast_rtx | partial_ack ;
 86
 87   assign rtx_start = wnd_start_in ;
 88   assign rtx_end = wnd_start_in + 1;
 89
 90   // reset rtx timer if not in recovery
 91   assign in_recovery _o ut = do_fast_rtx | ( i n_recove ry_in & cumula tive_ack <= recover_in );
 92   assign reset_rtx_t im er                = ~ i n_ re co v er y_ ou t ;
 93
 94
 95   assign in_timeout_o ut                                      = (~ full_ack ) & in_timeout_in ;
 96
 97   // // decide new window size
 98
 99   // keep a counter for additive increase
100   assign a d d i t i v e _ i n c _ c n t r _ o u t = in _ re co v er y_ ou t & ~ in_timeout_in ? 0 :
101                                                      is_new_ack & wnd_size_in >= ss_thresh_in ?
102                                                               ( a d d i t i v e _ i n c _ c n t r _ i n == wnd_size_in ? 0 :
103                                                                 a d d i t i v e _ i n c _ c n t r _ i n + 1) : a d d i t i v e _ i n c _ c n t r _ i n ;
104
105
106   assign wnd_size_out = new_wnd_size >= ‘ M A X _ W I N DO W _ S I Z E ? ‘ MA X _ W I N D O W _ S I Z E : new_wnd_size ;
107
108   always @ (*) begin
109     if ( do_fast_rtx ) begin
110       // set it equals to new ss_thresh , expanded for performance reasons
111       cwnd_out = sent_out - ‘ D U P _A C K S _ T H R E S H > 2 ? sent_out >> 1 : 1;
112     end
113     else if (~ in_re covery_ in & is_new_ack ) begin
114       if ( cwnd_in < ss_thresh_out ) begin
115          cwnd_out = cwnd_in + n ew ly _a c ke d_ c nt ;
116       end
117       else if ( wnd_ in c_ cn t r_ in >= cwnd_in ) begin
118          cwnd_out = cwnd_in + 1;
119       end
120       else begin
121          cwnd_out = cwnd_in ;
122       end
123     end
124     else begin
125       cwnd_out = cwnd_in ;
126     end
127   end
128   assign there_is_more = in_flight >= cwnd_in ;
129
130   always @ (*) begin
131     if ( do_fast_rtx ) begin
132       new_wnd_size = sent_out ;
133     end
134     else if (~ in_recovery_ in & is_new_ack ) begin
135       new_wnd_size = cwnd_out ;
136     end
137     else begin
138       new_wnd_size = there_is_more ? sent_out : cwnd_in + dup_acks ;
139     end
140   end
141
142   // // break up user context into variables
143   assign { prev_highest_ack_in , in_recovery_in , recover_in ,
144            in_timeout_in , wnd_inc_cntr_in , ss_thresh_in ,
145            dup_acks_in , cwnd_in } = user_cntxt_in ;
146
147   assign user_cntxt_o ut =               { prev_highest_ack_out , in_recovery_out , recover_out ,
148                                            in_timeout_out , wnd_inc_cntr_out , ss_thresh_out ,
149                                            dup_acks_outm , cwnd_out };
150
151
152   endmodule

